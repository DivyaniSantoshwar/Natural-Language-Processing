{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')     # tokenization\n",
    "nltk.download('stopwords') # stopword removal\n",
    "nltk.download('averaged_perceptron_tagger') # POS tagging\n",
    "nltk.download('wordnet')   # WordNet database and lemmatization\n",
    "nltk.download('omw-1.4')   # Stemming\n",
    "nltk.download('indian')    # Indian language POS tagging\n",
    "nltk.download('maxent_ne_chunker') # chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"They told that their ages are 25 27 and 31 respectively.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the average of ages mentioned in the above sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'told',\n",
       " 'that',\n",
       " 'their',\n",
       " 'ages',\n",
       " 'are',\n",
       " '25',\n",
       " '27',\n",
       " 'and',\n",
       " '31',\n",
       " 'respectively.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = sent.split(sep=\" \")\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "age =[]\n",
    "for x in lst:\n",
    "    if x.isdigit():\n",
    "        age.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 27, 31]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg = sum(age)/len(age)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages= [int(word) for word in sent.split() if word.isdigit()]\n",
    "sum(ages)/len(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#segmemtation\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word segmentation\n",
    "lst = word_tokenize(sent)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find percentage of punctuation symbol present in it\n",
    "punctuation_count = sum(1 for char in lst if char.isalnum() != True)\n",
    "punctuation_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(punctuation_count/len(lst))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_count1 = sum(1 for char in lst if not char.isalnum())\n",
    "punctuation_count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punch_count = len([char for char in word_tokenize(sent) if not char.isalnum()])\n",
    "punch_count/len(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(\"s1234567\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'×œ'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = '\\u0940'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¥€\n"
     ]
    }
   ],
   "source": [
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤µà¥€\n"
     ]
    }
   ],
   "source": [
    "char = '\\u0935\\u0940'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = '\\u0926\\u093F\\u092F\\u093E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤¦à¤¿à¤¯à¤¾\n"
     ]
    }
   ],
   "source": [
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['à¤¸à¤‚à¤¤à¥‹à¤·à¤µà¤¾à¤° à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¥€', 'à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤°  à¤…à¤—à¥à¤°à¤µà¤¾à¤²', 'à¤®à¤¾à¤¨à¤¸ ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤¸à¤‚à¤¤à¥‹à¤·à¤µà¤¾à¤° à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¥€\n",
      "à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤°  à¤…à¤—à¥à¤°à¤µà¤¾à¤²\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    if name.startswith('à¤¸'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¥€', 'à¤¸à¤‚à¤¤à¥‹à¤·à¤µà¤¾à¤°']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¶à¥€à¤µà¥à¤¯à¤¾à¤¨à¥€ à¤¸à¤‚à¤¤à¥‹à¤·à¤µà¤¾à¤°'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#name.replace( 'à¤¦à¤¿', 'à¤¶à¥€')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('à¤·')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtext = 'à¤°à¤¾à¤œà¤—à¤¢à¤¼ à¤à¤• à¤ªà¤¹à¤¾à¤¡à¤¼à¥€ à¤•à¤¿à¤²à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤­à¤¾à¤°à¤¤ à¤•à¥‡ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯ à¤•à¥‡ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤®à¥à¤°à¥à¤®à¤¦à¥‡à¤µ à¤•à¥‡ à¤¨à¤¾à¤® à¤¸à¥‡ à¤­à¥€ à¤œà¤¾à¤¨à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤•à¤¿à¤²à¤¾ à¤²à¤—à¤­à¤— 26 à¤µà¤°à¥à¤·à¥‹à¤‚ à¤¤à¤• à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤•à¥‡ à¤¶à¤¾à¤¸à¤¨ à¤®à¥‡à¤‚ à¤®à¤°à¤¾à¤ à¤¾ à¤¸à¤¾à¤®à¥à¤°à¤¾à¤œà¥à¤¯ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¥à¥€à¥¤ à¤¬à¤¾à¤¦ à¤®à¥‡à¤‚ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥‹ à¤°à¤¾à¤¯à¤—à¤¢à¤¼'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤°à¤¾à¤œà¤—à¤¢à¤¼',\n",
       " 'à¤à¤•',\n",
       " 'à¤ªà¤¹à¤¾à¤¡à¤¼à¥€',\n",
       " 'à¤•à¤¿à¤²à¤¾',\n",
       " 'à¤¹à¥ˆ',\n",
       " 'à¤œà¥‹',\n",
       " 'à¤­à¤¾à¤°à¤¤',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°',\n",
       " 'à¤°à¤¾à¤œà¥à¤¯',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤ªà¥à¤£à¥‡',\n",
       " 'à¤œà¤¿à¤²à¥‡',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¸à¥à¤¥à¤¿à¤¤',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤‡à¤¸à¥‡',\n",
       " 'à¤®à¥à¤°à¥à¤®à¤¦à¥‡à¤µ',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤¨à¤¾à¤®',\n",
       " 'à¤¸à¥‡',\n",
       " 'à¤­à¥€',\n",
       " 'à¤œà¤¾à¤¨à¤¾',\n",
       " 'à¤œà¤¾à¤¤à¤¾',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤¯à¤¹',\n",
       " 'à¤•à¤¿à¤²à¤¾',\n",
       " 'à¤²à¤—à¤­à¤—',\n",
       " '26',\n",
       " 'à¤µà¤°à¥à¤·à¥‹à¤‚',\n",
       " 'à¤¤à¤•',\n",
       " 'à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œ',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤¶à¤¾à¤¸à¤¨',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤®à¤°à¤¾à¤ à¤¾',\n",
       " 'à¤¸à¤¾à¤®à¥à¤°à¤¾à¤œà¥à¤¯',\n",
       " 'à¤•à¥€',\n",
       " 'à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€',\n",
       " 'à¤¥à¥€à¥¤',\n",
       " 'à¤¬à¤¾à¤¦',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€',\n",
       " 'à¤•à¥‹',\n",
       " 'à¤°à¤¾à¤¯à¤—à¤¢à¤¼']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤°à¤¾à¤œà¤—à¤¢à¤¼ à¤à¤• à¤ªà¤¹à¤¾à¤¡à¤¼à¥€ à¤•à¤¿à¤²à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤­à¤¾à¤°à¤¤ à¤•à¥‡ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯ à¤•à¥‡ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤®à¥à¤°à¥à¤®à¤¦à¥‡à¤µ à¤•à¥‡ à¤¨à¤¾à¤® à¤¸à¥‡ à¤­à¥€ à¤œà¤¾à¤¨à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤•à¤¿à¤²à¤¾ à¤²à¤—à¤­à¤— 26 à¤µà¤°à¥à¤·à¥‹à¤‚ à¤¤à¤• à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤•à¥‡ à¤¶à¤¾à¤¸à¤¨ à¤®à¥‡à¤‚ à¤®à¤°à¤¾à¤ à¤¾ à¤¸à¤¾à¤®à¥à¤°à¤¾à¤œà¥à¤¯ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¥à¥€à¥¤ à¤¬à¤¾à¤¦ à¤®à¥‡à¤‚ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥‹ à¤°à¤¾à¤¯à¤—à¤¢à¤¼'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**space tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open(r\"D:\\NLP\\day1\\mydata.txt\")\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "# create the object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAB TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello friendsðŸ™!ðŸ˜‰How are you?\\nWelcomeðŸ«´ to the worldðŸŒŽ of Python ðŸProgrammingðŸ’»â¤ï¸.'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "f = open(r\"D:\\NLP\\day1\\mydata.txt\")\n",
    "data = f.read()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!',\n",
       " 'How are you?\\nWelcome to the world of',\n",
       " 'Python Programming.']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TabTokenizer\n",
    "tbk = TabTokenizer()\n",
    "tbk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!\\tHow are you?',\n",
       " 'Welcome to the world of\\tPython Programming.']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import LineTokenizer\n",
    "ltk = LineTokenizer()\n",
    "ltk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wtk = WhitespaceTokenizer()\n",
    "wtk.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " 'o',\n",
       " ' ',\n",
       " 'f',\n",
       " 'r',\n",
       " 'i',\n",
       " 'e',\n",
       " 'n',\n",
       " 'd',\n",
       " 's',\n",
       " '!',\n",
       " '\\t',\n",
       " 'H',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " '?',\n",
       " '\\n',\n",
       " 'W',\n",
       " 'e',\n",
       " 'l',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'l',\n",
       " 'd',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " '\\t',\n",
       " 'P',\n",
       " 'y',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'P',\n",
       " 'r',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'm',\n",
       " 'm',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " '.']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwe = MWETokenizer()\n",
    "mwe.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Van Rossum is Python creator, visiting pune this week. the development communication is very eager to meet Van Rossum.\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'The Van Rossum is Python creator, visiting pune this week. the development communication is very eager to meet Van Rossum.'\n",
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'the',\n",
       " 'development',\n",
       " 'communication',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'the',\n",
       " 'development',\n",
       " 'communication',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwe = MWETokenizer(separator=' ')\n",
    "\n",
    "# add Multi World eXPRESSION\n",
    "mwe.add_mwe(('Van', 'Rossum'))\n",
    "\n",
    "mwe.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello friends :) ! How are you? Welcome to the world of\tPython Programming . ;) \n"
     ]
    }
   ],
   "source": [
    "sent = 'Hello friends :) ! How are you? Welcome to the world of\tPython Programming . ;) '\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " 'ðŸ™',\n",
       " '!',\n",
       " 'ðŸ˜‰',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'ðŸ«´',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'ðŸŒŽ',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'ðŸ',\n",
       " 'Programming',\n",
       " 'ðŸ’»',\n",
       " 'â¤',\n",
       " 'ï¸',\n",
       " '.']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "twt = TweetTokenizer()\n",
    "twt.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "Is\n",
      "it\n",
      "okay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\",text)\n",
    "text = \"This is some text with punctuation > Let's tokenize. Is it okay?\"\n",
    "tokens = custom_tokenizer(text)\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://mitu.co.in/dataset\n",
    "### download: student3.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"D:\\NLP\\day1\\student3.tsv\")\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'anil', 'TE', '56.77', '22'],\n",
       " ['2', 'amit', 'TE', '59.77', '21'],\n",
       " ['3', 'aniket', 'BE', '76.88', '19'],\n",
       " ['4', 'ajinkya', 'TE', '69.66', '20'],\n",
       " ['5', 'asha', 'TE', '63.28', '20'],\n",
       " ['6', 'ayesha', 'BE', '49.55', '20'],\n",
       " ['7', 'amar', 'BE', '65.34', '19'],\n",
       " ['8', 'amita', 'BE', '68.33', '23'],\n",
       " ['9', 'amol', 'TE', '56.75', '20'],\n",
       " ['10', 'anmol', 'BE', '78.66', '21'],\n",
       " ['']]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data.split(\"\\n\")\n",
    "x.pop(0)\n",
    "lst = []\n",
    "\n",
    "for i in x:\n",
    "        sub = i.split(\"\\t\")\n",
    "        lst.append(sub)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['roll', 'name', 'class', 'marks', 'age'],\n",
       " [1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21],\n",
       " ['']]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata = []\n",
    "for x in data.split('\\n'):\n",
    "    inner_list = []\n",
    "    for y in x.split('\\t'):\n",
    "        if y.isdigit():\n",
    "            inner_list.append(int(y))\n",
    "        elif y.find('.')>0 :\n",
    "            inner_list.append(float(y))\n",
    "        else:\n",
    "            inner_list.append(y)\n",
    "    newdata.append(inner_list)\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
